

<h1 align="center">Awesome-Affordance-Learning</h1>
<p align="center">
    <b> Curated collection of papers and resources on Affordance Learning.</b>
</p>


[![Awesome](https://awesome.re/badge.svg)](https://github.com/hq-King/Awesome-Affordance-Learning)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![Last Commit](https://img.shields.io/github/last-commit/hq-King/Awesome-Affordance-Learning?color=green)
![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-red)
![GitHub Repo stars](https://img.shields.io/github/stars/hq-King/Awesome-Affordance-Learning?style=social)


> üß≠ Exploring Embodied AI and Embodied perception ? We hope this collection proves useful in your journey. If you'd like to support the project, feel free to ‚≠êÔ∏è the repo and share it with your peers. Contributions are warmly welcome!

---

## üî• News

> üì¢ This list is **actively maintained**, and community contributions are always appreciated!  
> Feel free to [open a pull request](https://github.com/hq-King/Awesome-Affordance-Learning/pulls) if you find any relevant papers.

- üéâ `2025-05`: **Repository launched to curate a comprehensive list of Affordance Learning.**

---
- [üåü Introduction](#-introduction)
- [üìú Papers](#-papers)
  - [üí°2D Affordance Perception]
  - [üó£Ô∏è 3D Affordance Perception]
  - [üß™ Affordance Reasoning]
  - [üìö Affordance based Grasping & Manipulation]




---
## üåü Introduction
This repository offers a **comprehensive and up-to-date collection** of research papers on **Affordance Learning**.

> As robots(Embodied Agents) are increasingly integrated into real-world applications, their ability to manipulate objects is increasingly in demand, and affordance describes where and how to interact with objects.

This list spans across:
- 2D Affordance Perception
- 3D Affordance Perception
- Affordance Reasoning
- Affordance based Grasping & Manipulatation

  
Whether you're a researcher, developer, or enthusiast, this is your go-to hub for exploring Embodied perception.

---
## üìú Papers
### üí° 2D Affordance Perception
1. **AffordanceSAM: Segment Anything Once More in Affordance Grounding.**  
   *Dengyang Jiang, Mengmeng Wang, Teli Ma, Hengzhuang Li, Yong liu, Guang Dai, Lei Zhang.* [[abs](https://arxiv.org/abs/2504.15650)], Arxiv 2025.04

2. **GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation.**  
   *Teli Ma, Jia Zheng, Zifan Wang, Ziyao Gao, Jiaming Zhou, Junwei Liang.* [[abs](https://arxiv.org/abs/2505.11865)], Arxiv 2025.05

3. **One-Shot Open Affordance Learning with Foundation Models.**  
   *Gen Li, Deqing Sun, Laura Sevilla-Lara, Varun Jampani.* [[abs](https://arxiv.org/abs/2311.17776)], CVPR 2024

4. **Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors.**  
   *Peiran Xu, Yadong Mu.* [[abs](https://openreview.net/pdf?id=0823rvTIhs))], ICLR 2025

5. **AffordanceLLM: Grounding Affordance from Vision Language Models.**  
   *Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, Li Erran Li.* [[abs](https://arxiv.org/abs/2401.06341)], CVPRWS 2024

6. **Learning Affordance Grounding from Exocentric Images.**  
   *Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao.* [[abs](https://arxiv.org/abs/2203.09905)], CVPR 2022

7. **One-Shot Affordance Detection.**  
   *Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao.* [[abs](https://arxiv.org/abs/2106.14747)], IJCAI 2021

8. **WorldAfford: Affordance Grounding based on Natural Language Instructions.**  
   *Changmao Chen, Yuren Cong, Zhen Kan.* [[abs](https://arxiv.org/abs/2405.12461)], Arxiv 2024.05

9. **LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding.**  
   *Gen Li, Varun Jampani, Deqing Sun, Laura Sevilla-Lara.* [[abs](https://arxiv.org/abs/2303.09665)], CVPR 2023

10. **Affordance Grounding From Demonstration Video To Target Image.**  
   *Joya Chen, Difei Gao, Kevin Qinghong Lin, Mike Zheng Shou.* [[abs](https://arxiv.org/abs/2303.14644)], CVPR 2023
    
11. **One-Shot Transfer of Affordance Regions? AffCorrs!.**  
   *Denis Hadjivelichkov, Sicelukwanda Zwane, Marc Peter Deisenroth, Lourdes Agapito, Dimitrios Kanoulas.* [[abs](https://arxiv.org/abs/2303.09665)], CoRL 2023

12. **Text-driven Affordance Learning from Egocentric Vision.**  
   *Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori.* [[abs](https://arxiv.org/abs/2309.10932)], Arxiv 2023.09

13. **MaskPrompt: Open-Vocabulary Affordance Segmentation with Object Shape Mask Prompts.**  
   *Dongpan Chen, Dehui Kong, Jinghua Li, Baocai Yin.* [[abs](https://ojs.aaai.org/index.php/AAAI/article/view/32200)], AAAI 2025

14. **Reasoning Mamba: Hypergraph-Guided Region Relation Calculating for Weakly Supervised Affordance Grounding.**  
   *Yuxuan Wang, Aming Wu, Muli Yang, Yukuan Min, Yihang Zhu, Cheng Deng.* [[abs](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Reasoning_Mamba_Hypergraph-Guided_Region_Relation_Calculating_for_Weakly_Supervised_Affordance_CVPR_2025_paper.pdf)], CVPR 2025 

### üó£Ô∏è 3D Affordance Perception

1. **3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding.**  
   *Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, Kui Jia.* [[abs](https://arxiv.org/abs/2103.16397)], CVPR 2021

2. **Where2explore: Few-shot affordance learning for unseen novel categories of articulated objects.**  
   *Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, Hao Dong.* [[abs](https://arxiv.org/abs/2103.16397)], NeurIPS 2023
   
3. **LASO: Language-guided Affordance Segmentation on 3D Object.**  
   *Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, Tat-seng Chua.* [[abs](https://openaccess.thecvf.com/content/CVPR2024/html/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.html)], CVPR 2024

4. **SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes.**  
   *Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, Kui Jia.* [[abs](https://openaccess.thecvf.com/content/CVPR2024/html/Delitzas_SceneFun3D_Fine-Grained_Functionality_and_Affordance_Understanding_in_3D_Scenes_CVPR_2024_paper.html)], CVPR 2024

5. **Grounding 3D Object Affordance from 2D Interactions in Images.**  
   *Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, Zheng-Jun Zha.* [[abs](https://arxiv.org/abs/2303.10437)], ICCV 2023

6. **Learning 2d invariant affordance knowledge for 3d affordance grounding.**  
   *Xianqiang Gao, Pingrui Zhang, Delin Qu, Dong Wang, Zhigang Wang, Yan Ding, Bin Zhao.* [[abs](https://arxiv.org/abs/2408.13024)], AAAI 2025

7. **LEMON: Learning 3D Human-Object Interaction Relation from 2D Images.**  
   *Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Zheng-Jun Zha.* [[abs](https://arxiv.org/abs/2312.08963)], CVPR 2024

8. **Open-vocabulary affordance detection in 3d point clouds.**  
   *Toan Nguyen, Minh Nhat Vu, An Vuong, Dzung Nguyen, Thieu Vo, Ngan Le, Anh Nguyen.* [[abs](https://arxiv.org/abs/2303.02401)], IROS 2023

9. **Open-Vocabulary Affordance Detection using Knowledge Distillation and Text-Point Correlation.**  
   *Tuan Van Vo, Minh Nhat Vu, Baoru Huang, Toan Nguyen, Ngan Le, Thieu Vo, Anh Nguyen.* [[abs](https://arxiv.org/abs/2309.10932)], ICRA 2024

10. **3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds.**  
   *Hengshuo Chu, Xiang Deng, Qi Lv, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie.* [[abs](https://arxiv.org/abs/2502.20041)], ICLR 2025

11. **Grounding 3D Scene Affordance From Egocentric Interactions.**  
   *Cuiyu Liu, Wei Zhai, Yuhang Yang, Hongchen Luo, Sen Liang, Yang Cao, Zheng-Jun Zha.* [[abs](https://arxiv.org/abs/2409.19650)], Arxiv 2024.09

12. **Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions.**  
   *He Zhu, Quyu Kong, Kechun Xu, Xunlong Xia, Bing Deng, Jieping Ye, Rong Xiong, Yue Wang.* [[abs](https://arxiv.org/abs/2504.04744)], CVPR 2025

13. **GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency.**  
   *Dongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee.* [[abs](https://arxiv.org/abs/2412.09511)], CVPR 2025

14. **GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding.**  
   *Yawen Shao, Wei Zhai, Yuhang Yang, Hongchen Luo, Yang Cao, Zheng-Jun Zha.* [[abs](https://arxiv.org/abs/2411.19626)], CVPR 2025

15. **SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model.**  
   *Chunlin Yu, Hanqing Wang, Ye Shi, Haoyang Luo, Sibei Yang, Jingyi Yu, Jingya Wang.* [[abs](https://arxiv.org/abs/2412.01550)], CVPR 2025

16. **3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians.**  
   *Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin.* [[abs](https://arxiv.org/abs/2504.11218)], Arxiv 2025.04

17. **IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments.**  
   *Can Zhang, Gim Hee Lee.* [[abs](https://arxiv.org/abs/2504.06827)], CVPR 2025

18. **Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale.**  
   *Junha Lee, Eunha Park, Chunghyun Park, Dahyun Kang, Minsu Cho.* [[abs](https://arxiv.org/abs/2506.12009)], Arxiv 2025.06


### üß™ Affordance Reasoning

1. **AffordanceLLM: Grounding Affordance from Vision Language Models.**  
   *Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, Li Erran Li.* [[abs](https://arxiv.org/abs/2401.06341)], CVPRWS 2024

2. **SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model.**  
   *Chunlin Yu, Hanqing Wang, Ye Shi, Haoyang Luo, Sibei Yang, Jingyi Yu, Jingya Wang.* [[abs](https://arxiv.org/abs/2412.01550)], CVPR 2025

3. **3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds.**  
   *Hengshuo Chu, Xiang Deng, Qi Lv, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie.* [[abs](https://arxiv.org/abs/2502.20041)], ICLR 2025
   
4. **Affordance Benchmark for MLLMs.**  
   *Junying Wang, Wenzhe Li, Yalun Wu, Yingji Liang, Yijin Guo, Chunyi Li, Haodong Duan, Zicheng Zhang, Guangtao Zhai.* [[abs](https://arxiv.org/abs/2506.00893)], Arxiv 25.06
    
### üìö Affordance based Grasping & Manipulation

1. **OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding.**  
   *Edmond Tong, Anthony Opipari, Stanley Lewis, Zhen Zeng, Odest Chadwicke Jenkins.* [[abs](https://arxiv.org/abs/2404.11000)], Arxiv 24.04

2. **Learning 6-DoF Task-oriented Grasp Detection via Implicit Estimation and Visual Affordance.**  
   *Edmond Tong, Anthony Opipari, Stanley Lewis, Zhen Zeng, Odest Chadwicke Jenkins.* [[abs](https://ieeexplore.ieee.org/abstract/document/9981900)], Iros 2022

3. **Affordance-Driven Next-Best-View Planning for Robotic Grasping.**  
   *Xuechao Zhang, Dong Wang, Sun Han, Weichuang Li, Bin Zhao, Zhigang Wang, Xiaoming Duan, Chongrong Fang, Xuelong Li, Jianping He.* [[abs](https://arxiv.org/abs/2309.09556)], Arxiv 23.09
   
4. **Learning Generalizable Dexterous Manipulation from Human Grasp Affordance.**  
   *Yueh-Hua Wu, Jiashun Wang, Xiaolong Wang.* [[abs](https://arxiv.org/abs/2204.02320)], Arxiv 22.04

5. **UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation.**  
   *Yihe Tang1, Wenlong Huang1, Yingke Wang1, Chengshu Li1, Roy Yuan1, Ruohan Zhang1, Jiajun Wu1, Li Fei-Fei1.* [[abs](https://gpt-affordance.github.io/)], ICRA 2025

6. **AffordDP: Generalizable Diffusion Policy with Transferable Affordance.**  
   *Shijie Wu, Yihang Zhu, Yunao Huang, Kaizhen Zhu, Jiayuan Gu, Jingyi Yu, Ye Shi, Jingya Wang.* [[abs](https://arxiv.org/abs/2412.03142)], CVPR 2025

7. **GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation.**  
   *Teli Ma, Jia Zheng, Zifan Wang, Ziyao Gao, Jiaming Zhou, Junwei Liang.* [[abs](https://arxiv.org/abs/2505.11865)], Arxiv 2025.05

8. **GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping.**  
   *Teli Ma, Zifan Wang, Jiaming Zhou, Mengmeng Wang, Junwei Liang.* [[abs](https://arxiv.org/abs/2505.11865)], Arxiv 2024.11

9. **BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly.**  
   *Yan Shen, Ruihai Wu, Yubin Ke, Xinyuan Song, Zeyi Li, Xiaoqi Li, Hongwei Fan, Haoran Lu, Hao Dong.* [[abs](https://sites.google.com/view/biassembly)], ICML 2025

10. **GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation.**  
   *Ruihai Wu, Ziyu Zhu, Yuran Wang, Yue Chen, Jiarui Wang, Hao Dong.* [[abs](https://arxiv.org/pdf/2503.09243)], CVPR 2025

11. **NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation.**  
   *Ran Xu, Yan Shen, Xiaoqi Li, Ruihai Wu, Hao Dong.* [[abs](https://arxiv.org/pdf/2403.08355)], RAL 2024

12. **ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models.**  
   *Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xiaoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, Hao Dong.* [[abs](https://arxiv.org/pdf/2403.08355)], IROS 2024

13. **Learning Environment-aware Affordance for 3D Articulated Object Manipulation under Occlusions.**  
   *Ruihai Wu, Kai Cheng, Yan Zhao, Chuanruo Ning, Guanqi Zhan, Hao Dong.* [[abs](https://arxiv.org/abs/2309.07510)], NeurIPS 2023

14. **DefoAfford: Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation.**  
   *Ruihai Wu, Chuanruo Ning, Hao Dong.* [[abs](https://arxiv.org/abs/2303.11057)], ICCV 2023

15. **RLAfford: End-to-End Affordance Learning for Robotic Manipulation.**  
   *Yiran Geng, Boshi An, Haoran Geng, Yuanpei Chen, Yaodong Yang, Hao Dong.* [[abs](https://arxiv.org/pdf/2209.12941)], ICRA 2023

16. **DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation.**  
   *Yan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang, Qingnan Fan, Kaichun Mo, Hao Dong.* [[abs](https://arxiv.org/pdf/2207.01971)], ICLR 2023

16. **Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation.**  
   *Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, Huazhe Xu.* [[abs](https://arxiv.org/abs/2401.07487)], ECCV 2024

17. **AffordDexGrasp: Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordanc.**  
   *Yi-Lin Wei, Mu Lin, Yuhao Lin, Jian-Jian Jiang, Xiao-Ming Wu, Ling-An Zeng, Wei-Shi Zheng* [[abs](https://arxiv.org/abs/2503.07360)], Arxiv 25.03

18. **Learning Precise Affordances from Egocentric Videos for Robotic Manipulation.**  
   *Gen Li, Nikolaos Tsagkas, Jifei Song, Ruaridh Mon-Williams, Sethu Vijayakumar, Kun Shao, Laura Sevilla-Lara* [[abs](https://arxiv.org/abs/2408.10123)], Arxiv 24.08

19. **DORA: Object Affordance-Guided Reinforcement Learning for Dexterous Robotic Manipulation.**  
   *Lei Zhang, Soumya Mondal, Zhenshan Bing, Kaixin Bai, Diwen Zheng, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang* [[abs](https://arxiv.org/abs/2505.14819)], Arxiv 25.05

20. **A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation.**  
   *Rongtao Xu, Jian Zhang, Minghao Guo, Youpeng Wen, Haoting Yang, Min Lin, Jianzheng Huang, Zhe Li, Kaidong Zhang, Liqiong Wang, Yuxuan Kuang, Meng Cao, Feng Zheng, Xiaodan Liang* [[abs](https://arxiv.org/abs/2504.12636)], Arxiv 25.04
    
21. **BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly.**  
   *Yan Shen, Ruihai Wu, Yubin Ke, Xinyuan Song, Zeyi Li, Xiaoqi Li, Hongwei Fan, Haoran Lu, Hao dong* [[abs](https://arxiv.org/abs/2506.06221)], ICML 2025
---

## üéâ Contributing
‚≠ê Help us grow this repository! If you know any valuable works we‚Äôve missed, don‚Äôt hesitate to contribute ‚Äî every suggestion makes a difference!

We welcome and appreciate all contributions! Here‚Äôs how you can help:

- üìÑ **Add or Update a Paper**  
  Contribute by adding a new paper or improving details of an existing one. Please consider the most appropriate category for the work.

- ‚úçÔ∏è **Use Consistent Formatting**  
  Follow the format of the existing entries to maintain clarity and consistency across the list.

- üîó **Include Abstract Link**  
  If the paper is from arXiv, use the `/abs/` link format for the abstract (e.g., `https://arxiv.org/abs/xxxx.xxxxx`).

- üí° **Explain Your Edit (Optional but Helpful)**  
  A short note on why you think the paper deserves to be added or updated is appreciated and helps maintainers process your PR faster.

> **‚úÖ Don't worry about getting everything perfect!**  
> Minor mistakes are totally fine ‚Äî we‚Äôll help fix them. What matters most is your contribution. Let's highlight your awesome work together!

---
## Acknowledgement
Thanks for the wonderful project: [Awesome-LLM-Empathy](https://github.com/JhCircle/Awesome-LLM-Empathy). This project is built upon it.

## üìÑ License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).

## Contributors

<a href="https://github.com/hq-King/Awesome-Affordance-Learning/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=hq-King/Awesome-Affordance-Learning" />
</a>
