# Awosome-Affordance-List


[![Awesome](https://awesome.re/badge.svg)](https://github.com/JhCircle/Awesome-LLM-Empathy)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![Last Commit](https://img.shields.io/github/last-commit/hq-King/Awesome-Affordance-List?color=green)
![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-red)
![GitHub Repo stars](https://img.shields.io/github/stars/hq-King/Awesome-Affordance-List?style=social)
> ðŸ§­ Exploring Embodied AI and Embodied perception ? We hope this collection proves useful in your journey. If you'd like to support the project, feel free to â­ï¸ the repo and share it with your peers. Contributions are warmly welcome!

---

## ðŸ”¥ News

> ðŸ“¢ This list is **actively maintained**, and community contributions are always appreciated!  
> Feel free to [open a pull request](https://github.com/JhCircle/Awesome-LLM-Empathy/pulls) if you find any relevant papers.

- ðŸŽ‰ `2025-05`: **Our paper [_ReflectDiffu_](https://arxiv.org/abs/2409.10289) was accepted at ACL 2025 Main!**
- ðŸŽ‰ `2025-05`: **Repository launched to curate a comprehensive list of Empathy-focused LM research.**

---
- [ðŸŒŸ Introduction](#-introduction)
- [ðŸ“œ Papers](#-papers)
  - [ðŸ’¡2D Affordance Perception]
  - [ðŸ—£ï¸ Empathetic Dialogue Systems]
  - [ðŸ§ª Affordance Reasoning]
  - [ðŸ“š Affordance based Grasping & Manipulatation]
  - [ðŸ“„ Surveys & Overviews]




---
## ðŸŒŸ Introduction
This repository offers a **comprehensive and up-to-date collection** of research papers on **Affordance perception**.

> As robots(Embodied Agents) are increasingly integrated into real-world applications, their ability to manipulate objects is increasingly in demand, and affordance describes where and how to interact with objects.

This list spans across:
- 2D Affordance Perception
- 3D Affordance Perception
- Affordance Reasoning
- Affordance based Grasping & Manipulatation
Whether you're a researcher, developer, or enthusiast, this is your go-to hub for exploring Embodied perception.

---
## ðŸ“œ Papers
### ðŸ’¡ 2D Affordance Perception
1. **AffordanceSAM: Segment Anything Once More in Affordance Grounding.**  
   *Dengyang Jiang, Mengmeng Wang, Teli Ma, Hengzhuang Li, Yong liu, Guang Dai, Lei Zhang.* [[abs](https://arxiv.org/abs/2504.15650)], Arxiv 2025.04

2. **GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation.**  
   *Teli Ma, Jia Zheng, Zifan Wang, Ziyao Gao, Jiaming Zhou, Junwei Liang.* [[abs](https://arxiv.org/abs/2505.11865)], Arxiv 2025.05

3. **One-Shot Open Affordance Learning with Foundation Models.**  
   *Gen Li, Deqing Sun, Laura Sevilla-Lara, Varun Jampani.* [[abs](https://arxiv.org/abs/2311.17776)], CVPR 2024

4. **Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors.**  
   *Peiran Xu, Yadong Mu.* [[abs](https://openreview.net/pdf?id=0823rvTIhs))], ICLR 2025

5. **AffordanceLLM: Grounding Affordance from Vision Language Models.**  
   *Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, Li Erran Li.* [[abs](https://arxiv.org/abs/2401.06341)], CVPRWS 2024

6. **Learning Affordance Grounding from Exocentric Images.**  
   *Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao.* [[abs](https://arxiv.org/abs/2203.09905)], CVPR 2022

7. **One-Shot Affordance Detection.**  
   *Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao.* [[abs](https://arxiv.org/abs/2106.14747)], IJCAI 2021

8. **WorldAfford: Affordance Grounding based on Natural Language Instructions.**  
   *Changmao Chen, Yuren Cong, Zhen Kan.* [[abs](https://arxiv.org/abs/2405.12461)], Arxiv 2024.05

9. **LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding.**  
   *Gen Li, Varun Jampani, Deqing Sun, Laura Sevilla-Lara.* [[abs](https://arxiv.org/abs/2303.09665)], CVPR 2023

10. **Affordance Grounding From Demonstration Video To Target Image.**  
   *Joya Chen, Difei Gao, Kevin Qinghong Lin, Mike Zheng Shou.* [[abs](https://arxiv.org/abs/2303.14644)], CVPR 2023
    
11. **One-Shot Transfer of Affordance Regions? AffCorrs!.**  
   *Denis Hadjivelichkov, Sicelukwanda Zwane, Marc Peter Deisenroth, Lourdes Agapito, Dimitrios Kanoulas.* [[abs](https://arxiv.org/abs/2303.09665)], CoRL 2023

12. **Text-driven Affordance Learning from Egocentric Vision.**  
   *Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori.* [[abs](https://arxiv.org/abs/2309.10932)], Arxiv 2023.09

13. **MaskPrompt: Open-Vocabulary Affordance Segmentation with Object Shape Mask Prompts.**  
   *Dongpan Chen, Dehui Kong, Jinghua Li, Baocai Yin. [[abs](https://ojs.aaai.org/index.php/AAAI/article/view/32200)], AAAI 2025

    

### ðŸ—£ï¸ 3D Affordance Perception

1. **3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding.**  
   *Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, Kui Jia.* [[abs](https://arxiv.org/abs/2103.16397)], CVPR 2021

2. **Where2explore: Few-shot affordance learning for unseen novel categories of articulated objects.**  
   *Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, Hao Dong.* [[abs](https://arxiv.org/abs/2103.16397)], NeurIPS 2023
   
3. **LASO: Language-guided Affordance Segmentation on 3D Object.**  
   *Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, Tat-seng Chua.* [[abs](https://openaccess.thecvf.com/content/CVPR2024/html/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.html)], CVPR 2024

4. **SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes.**  
   *Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, Kui Jia.* [[abs](https://openaccess.thecvf.com/content/CVPR2024/html/Delitzas_SceneFun3D_Fine-Grained_Functionality_and_Affordance_Understanding_in_3D_Scenes_CVPR_2024_paper.html)], CVPR 2024

5. **Grounding 3D Object Affordance from 2D Interactions in Images.**  
   *Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Jiebo Luo, Zheng-Jun Zha.* [[abs](https://arxiv.org/abs/2303.10437)], ICCV 2023

6. **Learning 2d invariant affordance knowledge for 3d affordance grounding.**  
   *Xianqiang Gao, Pingrui Zhang, Delin Qu, Dong Wang, Zhigang Wang, Yan Ding, Bin Zhao.* [[abs](https://arxiv.org/abs/2408.13024)], AAAI 2025

7. **LEMON: Learning 3D Human-Object Interaction Relation from 2D Images.**  
   *Yuhang Yang, Wei Zhai, Hongchen Luo, Yang Cao, Zheng-Jun Zha.* [[abs](https://arxiv.org/abs/2312.08963)], CVPR 2024

8. **Open-vocabulary affordance detection in 3d point clouds.**  
   *Toan Nguyen, Minh Nhat Vu, An Vuong, Dzung Nguyen, Thieu Vo, Ngan Le, Anh Nguyen.* [[abs](https://arxiv.org/abs/2303.02401)], IROS 2023

9. **Open-Vocabulary Affordance Detection using Knowledge Distillation and Text-Point Correlation.**  
   *Tuan Van Vo, Minh Nhat Vu, Baoru Huang, Toan Nguyen, Ngan Le, Thieu Vo, Anh Nguyen.* [[abs](https://arxiv.org/abs/2309.10932)], ICRA 2024

10. **3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds.**  
   *Hengshuo Chu, Xiang Deng, Qi Lv, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie.* [[abs](https://arxiv.org/abs/2502.20041)], ICLR 2025

11. **Grounding 3D Scene Affordance From Egocentric Interactions.**  
   *Cuiyu Liu, Wei Zhai, Yuhang Yang, Hongchen Luo, Sen Liang, Yang Cao, Zheng-Jun Zha.* [[abs](https://arxiv.org/abs/2409.19650)], Arxiv 2024.09

12. **Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions.**  
   *He Zhu, Quyu Kong, Kechun Xu, Xunlong Xia, Bing Deng, Jieping Ye, Rong Xiong, Yue Wang.* [[abs](https://arxiv.org/abs/2504.04744)], CVPR 2025

13. **GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency.**  
   *Dongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee.* [[abs](https://arxiv.org/abs/2412.09511)], CVPR 2025

14. **GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding.**  
   *Yawen Shao, Wei Zhai, Yuhang Yang, Hongchen Luo, Yang Cao, Zheng-Jun Zha.* [[abs](https://arxiv.org/abs/2411.19626)], CVPR 2025

15. **SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model.**  
   *Chunlin Yu, Hanqing Wang, Ye Shi, Haoyang Luo, Sibei Yang, Jingyi Yu, Jingya Wang.* [[abs](https://arxiv.org/abs/2412.01550)], CVPR 2025


16. **3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians.**  
   *Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin.* [[abs](https://arxiv.org/abs/2504.11218)], Arxiv 2025.04




### ðŸ§ª Affordance Reasoning


### ðŸ“š Affordance based Grasping & Manipulatation


### ðŸ“„ Surveys & Overviews

---

## ðŸŽ‰ Contributing
â­ Help us grow this repository! If you know any valuable works weâ€™ve missed, donâ€™t hesitate to contribute â€” every suggestion makes a difference!

We welcome and appreciate all contributions! Hereâ€™s how you can help:

- ðŸ“„ **Add or Update a Paper**  
  Contribute by adding a new paper or improving details of an existing one. Please consider the most appropriate category for the work.

- âœï¸ **Use Consistent Formatting**  
  Follow the format of the existing entries to maintain clarity and consistency across the list.

- ðŸ”— **Include Abstract Link**  
  If the paper is from arXiv, use the `/abs/` link format for the abstract (e.g., `https://arxiv.org/abs/xxxx.xxxxx`).

- ðŸ’¡ **Explain Your Edit (Optional but Helpful)**  
  A short note on why you think the paper deserves to be added or updated is appreciated and helps maintainers process your PR faster.

> **âœ… Don't worry about getting everything perfect!**  
> Minor mistakes are totally fine â€” weâ€™ll help fix them. What matters most is your contribution. Let's highlight your awesome work together!

---
## ðŸ™Œ Citation

## ðŸ“„ License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT).
